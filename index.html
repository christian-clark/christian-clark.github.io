<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
    <title>
      Christian Clark
    </title>
  </head>

  <body>
    <div class="content">
      <div class="row">
        <div class="column" style="flex:22%; width:150px;">
          <img style="width:75%; border-radius:50%;"  src="photo.jpg">
        </div>
        <div class="column" style="margin-left:-5px; flex:78%;">
          <h1 style="margin-top:0px; margin-bottom:10px">Christian Clark</h1>
          <div class="info" style="margin-bottom: 0px; margin-top: 0px">
            <p>lastname DOT 3664 AT osu</p>
            <p><a href="https://scholar.google.com/citations?user=TLZ286oAAAAJ&hl=en">Google Scholar</a> / <a href="https://github.com/christian-clark">Github</a> / <a href="https://www.linkedin.com/in/christian-clark-465365113/">LinkedIn</a></p>
          </div>
        </div>
      </div>

      [<b><a href="vitae.pdf">Curriculum Vitae</a></b>]
      
      <p>I am a PhD candidate in the <a href="https://linguistics.osu.edu/">Ohio State Linguistics Department</a>, working with <a href="https://www.asc.ohio-state.edu/schuler.77/">William Schuler</a>. 
      I am interested in fundamental questions about language acquisition and comprehension: How do children pick up linguistic structure from the limited input they receive, and how do language users piece together complex meanings from sentences they hear or read? My research studies these questions using broad-coverage computational models that draw from advances in natural language processing.</p>

    <hr>
    <h2>Publications</h2>
      <ul class="pubs">
        <li>
          How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?
          <br><b>Christian Clark</b>, Byung-Doh Oh, and William Schuler,
          <br><i>arXiv preprint</i>, 2025.
          <br>[<a href="https://arxiv.org/pdf/2507.22209">paper</a>][<a href="https://github.com/christian-clark/word-entropy">code</a>][<a href="bib/arxiv25bib.html">bibtex</a>]
        </li>
        <li>
          Linear Recency Bias During Training Improves Transformers' Fit to Reading Times.
          <br><b>Christian Clark</b>, Byung-Doh Oh, and William Schuler,
          <br><i>Proceedings of the 31st International Conference on Computational Linguistics</i>, 2025.
          <br>[<a href="https://aclanthology.org/2025.coling-main.517.pdf">paper</a>][<a href="https://github.com/christian-clark/recency-bias">code</a>][<a href="bib/coling25bib.html">bibtex</a>]
        </li>
        <li>
          Categorial Grammar Induction with Stochastic Category Selection.
          <br><b>Christian Clark</b> and William Schuler,
          <br><i>Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</i>, 2024.
          <br>[<a href="https://aclanthology.org/2024.lrec-main.258.pdf">paper</a>][<a href="https://github.com/christian-clark/cgInduction">code</a>][<a href="bib/coling24bib.html">bibtex</a>]
        </li>
        <li>
          Categorial Grammar Induction from Raw Data.
          <br><b>Christian Clark</b> and William Schuler,
          <br><i>Findings of the Association for Computational Linguistics</i>, 2023.
          <br>[<a href="https://aclanthology.org/2023.findings-acl.149.pdf">paper</a>][<a href="https://github.com/christian-clark/cgInduction">code</a>][<a href="bib/acl23bib.html">bibtex</a>]
        </li>
        <li>
          Comparison of Structural Parsers and Neural Language Models as Surprisal Estimators.
          <br>Byung-Doh Oh, <b>Christian Clark</b>, and William Schuler,
          <br><i>Frontiers in Artificial Intelligence</i>, 2022.
          <br>[<a href="https://www.frontiersin.org/articles/10.3389/frai.2022.777963/full">paper</a>][<a href="https://github.com/byungdoh/acl21_semproc">code</a>][<a href="bib/frontiers22bib.html">bibtex</a>]
        </li>
        <li>
          Surprisal Estimators for Human Reading Times Need Character Models.
          <br>Byung-Doh Oh, <b>Christian Clark</b>, and William Schuler,
          <br><i>The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</i>, 2021.
          <br>[<a href="https://aclanthology.org/2021.acl-long.290.pdf">paper</a>][<a href="https://github.com/byungdoh/acl21_semproc">code</a>][<a href="bib/acl21bib.html">bibtex</a>]
        </li>
      </ul>

    <h2>Presentations</h2>
      <ul class="pubs">
        <li>
          <i>Effects of Recency Bias on Transformers' Predictions of Reading Times.</i>
          Invited talk (virtual) at University of Nottingham, 2025.
        </li>
        <li>
          <i>Linear Recency Bias During Training Improves Transformers' Fit to Reading Times.</i>
           Poster presentation at Midwest Speech and Language Days (MSLD), 2025.
        </li>
        <li>
          <i>Effects of Recency Bias on Transformers' Predictions of Reading Times.</i>
          Oral presentation at the 38th Annual Conference on Human Sentence Processing, 2025.
          <br>[<a href="https://hsp2025.github.io/abstracts/189.pdf">abstract</a>][<a href="hsp2025slides.pdf">slides</a>]
        </li>
        <li>
          <i>Categorial Grammar Induction from Raw Data.</i>
           Short talk and poster presentation at Midwest Speech and Language Days (MSLD), 2024.
        </li>
        <li>
          <i>Introduction to Computational Linguistics and Language Modeling.</i>
          Invited guest lecture for Ohio State Summer Linguistics Institute for Youth Scholars (SLIYS), 2023 and 2024.
        </li>
        <li>
          <i>Evidence for Composition Operations in Broad-Coverage Sentence Processing.</i>
          Poster presentation at the Ohio State Center for Cognitive and Brain Sciences Fall Retreat, 2022.
        </li>
        <li>
          <i>Evidence for Composition Operations in Broad-Coverage Sentence Processing.</i>
          Poster presentation (virtual) at the 35th Annual Conference on Human Sentence Processing</i>, 2022.
          <br>[<a href="https://oxford-abstracts.s3.amazonaws.com/6d8888b3-85bd-42b0-b846-ab09b391508a.pdf">abstract</a>]
        </li>
        <li>
          <i>LaTeX for Linguists.</i>
          Ohio State Language and Computing Committee tutorial, offered annually from 2020&ndash;2024.
          <br>[<a href="https://www.ling.ohio-state.edu/LCC/latex-tutorial/beginner/">tutorial</a>]
        </li>
        <li>
          <i>Data Selection for Language Modeling.</i>
          Workshop presentation at Amazon Machine Learning Conference, 2018.
        </li>
      </ul>

    <h2>Teaching</h2>
      <ul class="pubs">
        <li>
          Graduate Teaching Associate (Instructor of Record),
          <br>LING 3802: Language and Computers,
          <br>The Ohio State University, 2022&ndash;2023
        </li>
        <li>
          Undergraduate Teaching Assistant,
          <br>CS 0008: Introduction to Python Programming,
          <br>University of Pittsburgh, 2015&ndash;2016
        </li>
      </ul>

    <h2>Awards</h2>
      <ul>
        <li>2020 &mdash; Dean's Distinguished University Fellowship, The Ohio State University</li>
        <li>2017 &mdash; Outstanding Undergraduate Student, University of Pittsburgh Computer Science Department</li>
        <li>2013 &mdash; Chancellor's Scholarship, University of Pittsburgh</li>
      </ul>

    <h2>Personal</h2>
    
    I worked as an Applied Scientist for Amazon Alexa before graduate school.
    In my free time I enjoy backgammon and classical music.
    One of my more eccentric undertakings has been building a <a href="organ.html">custom digital organ</a> in my apartment.

    </div>
  </body>
</html>
